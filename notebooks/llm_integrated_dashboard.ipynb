{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# # Retail Analytics Dashboard (llm_ml_integrated_dashboard.ipynb)\n",
        "\n",
        "- **ML Predictions**: Linear Regression, XGBoost, Random Forest, ARIMA, LSTM models from `ml_model_training.ipynb`.\n",
        "# - **BI Insights**: LLM-powered queries via OpenRouter API, analyzing `pp_df_data.csv` (100k preprocessed retail sales records, India stores, 2019-2023).\n",
        "# - **Visualizations**: Dynamic charts for predictions and top metrics.\n",
        "#\n",
        "# **Dataset**: `pp_df_data.csv` (from `pp_fe_eda.ipynb`)  \n",
        "# **Models**: Stored in `models/` (from `ml_model_training.ipynb`)  \n",
        "# **Output**: `app.py`, `requirements.txt`â€”runs locally or in Colab with Ngrok URL  \n",
        "#\n",
        "# **Instructions**:\n",
        "# 1. Upload `pp_df_data.csv` and ensure models are in `models/` (or Google Drive).\n",
        "# 2. Set OpenRouter API key and Ngrok token in Streamlit secrets (Colab) or `.env` (local).\n",
        "# 3. Run all cells to launch the dashboard."
      ],
      "metadata": {
        "id": "2NdHEJaStJMn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sv5Fp3uupmDn"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install requests streamlit==1.29.0 pyngrok==7.2.0 altair==5.4.1 tensorflow==2.17.0 scikit-learn==1.5.2 xgboost==2.1.1 statsmodels==0.14.3 -q\n",
        "print(\"Dependencies installedâ€”ready to roll!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "\n",
        "# Create project directories\n",
        "os.makedirs('models', exist_ok=True)\n",
        "os.makedirs('data', exist_ok=True)\n",
        "\n",
        "# Upload preprocessed data\n",
        "print(\"Upload pp_df_data.csv\")\n",
        "uploaded = files.upload()\n",
        "for filename in uploaded.keys():\n",
        "    os.rename(filename, 'data/pp_df_data.csv')\n",
        "print(\"Data uploadedâ€”models expected in 'models/'\")"
      ],
      "metadata": {
        "id": "GK-GMwjwt91t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Create Streamlit App"
      ],
      "metadata": {
        "id": "QpFvWamct_Sd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import streamlit as st\n",
        "import requests\n",
        "import altair as alt\n",
        "import tensorflow as tf\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set page config\n",
        "st.set_page_config(page_title=\"Retail Analytics Dashboard\", page_icon=\"ðŸ“ˆ\", layout=\"wide\")\n",
        "\n",
        "# Configuration\n",
        "DATA_PATH = \"data/pp_df_data.csv\"\n",
        "MODEL_DIR = \"models\"\n",
        "OPENROUTER_API_KEY = st.secrets.get(\"OPENROUTER_API_KEY\", \"your-api-key\")\n",
        "API_URL = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "HEADERS = {\n",
        "    \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Model and scaler paths\n",
        "MODEL_PATHS = {\n",
        "    \"Linear Regression\": f\"{MODEL_DIR}/linear_regression_model.pkl\",\n",
        "    \"XGBoost\": f\"{MODEL_DIR}/xgboost_model.pkl\",\n",
        "    \"Random Forest\": f\"{MODEL_DIR}/random_forest_model.pkl\",\n",
        "    \"ARIMA\": f\"{MODEL_DIR}/arima_model.pkl\",\n",
        "    \"LSTM\": f\"{MODEL_DIR}/lstm_model.h5\"\n",
        "}\n",
        "SCALER_PATHS = {\n",
        "    \"Linear Regression\": f\"{MODEL_DIR}/scaler_lr.pkl\",\n",
        "    \"XGBoost\": f\"{MODEL_DIR}/scaler_xgb.pkl\",\n",
        "    \"Random Forest\": f\"{MODEL_DIR}/scaler_lr.pkl\"  # Shared scaler\n",
        "}\n",
        "\n",
        "# Column aliases for BI queries\n",
        "COLUMN_ALIASES = {\n",
        "    \"sales\": \"Sales\", \"profit\": \"Profit\", \"quantity\": \"Quantity\", \"discount\": \"Discount\",\n",
        "    \"clv\": \"CLV\", \"age\": \"Customer Age\", \"fulfillment\": \"Fulfillment Time\",\n",
        "    \"popularity\": \"Product Popularity\", \"discount impact\": \"Discount Impact\",\n",
        "    \"year\": \"Year\", \"region\": \"Region\", \"state\": \"State\", \"category\": \"Category of Goods\",\n",
        "    \"subcategory\": \"Sub-Category\", \"product\": \"Product Name\", \"customer\": \"Customer Name\",\n",
        "    \"id\": \"Customer ID\", \"order\": \"Order ID\", \"ship\": \"Ship Mode\"\n",
        "}\n",
        "\n",
        "# Utility functions\n",
        "def load_data():\n",
        "    \"\"\"Load preprocessed retail sales data.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(DATA_PATH)\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        st.error(\"Error: pp_df_data.csv not found in data/â€”please upload it.\")\n",
        "        st.stop()\n",
        "\n",
        "def load_models_and_scalers():\n",
        "    \"\"\"Load ML models and scalers from models/ directory.\"\"\"\n",
        "    models = {}\n",
        "    scalers = {}\n",
        "    for name, path in MODEL_PATHS.items():\n",
        "        try:\n",
        "            if path.endswith('.pkl'):\n",
        "                models[name] = pickle.load(open(path, \"rb\"))\n",
        "            elif path.endswith('.h5'):\n",
        "                models[name] = tf.keras.models.load_model(path, compile=False)\n",
        "        except Exception as e:\n",
        "            st.warning(f\"Failed to load model {name}: {str(e)}\")\n",
        "    for name, path in SCALER_PATHS.items():\n",
        "        try:\n",
        "            scalers[name] = pickle.load(open(path, \"rb\"))\n",
        "        except Exception as e:\n",
        "            st.warning(f\"Failed to load scaler for {name}: {str(e)}\")\n",
        "    return models, scalers\n",
        "\n",
        "def find_closest_row(data, quantity, discount, fulfillment_time):\n",
        "    \"\"\"Find the closest row in data based on input features.\"\"\"\n",
        "    data['distance'] = np.sqrt(\n",
        "        (data['Quantity'] - quantity) ** 2 +\n",
        "        (data['Discount'] - discount) ** 2 +\n",
        "        (data['Fulfillment Time'] - fulfillment_time) ** 2\n",
        "    )\n",
        "    return data.loc[data['distance'].idxmin()]\n",
        "\n",
        "def preprocess_input(row, scaler):\n",
        "    \"\"\"Preprocess input features for ML models.\"\"\"\n",
        "    features = ['Quantity', 'Discount', 'Fulfillment Time', 'Customer Age', 'CLV',\n",
        "                'Discount Impact', 'Product Popularity', 'Year', 'Postal Code']\n",
        "    input_features = row[features].values.reshape(1, -1)\n",
        "    return scaler.transform(input_features)\n",
        "\n",
        "def preprocess_lstm_input(quantity, discount, fulfillment_time, timesteps=6):\n",
        "    \"\"\"Preprocess input for LSTM model.\"\"\"\n",
        "    input_features = np.array([[quantity, discount, fulfillment_time]] * timesteps)\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_input = scaler.fit_transform(input_features)\n",
        "    return scaled_input.reshape(1, timesteps, 3), scaler\n",
        "\n",
        "def process_query(query, df, models, scalers, inputs=None):\n",
        "    \"\"\"Process BI query or ML prediction using LLM.\"\"\"\n",
        "    query = query.lower().strip()\n",
        "    action = \"predict\" if inputs else (\"top\" if \"top\" in query else \"summary\")\n",
        "    metric = None\n",
        "    region = None\n",
        "    n = 5\n",
        "\n",
        "    # Parse query\n",
        "    words = query.split()\n",
        "    for i, word in enumerate(words):\n",
        "        if word.isdigit():\n",
        "            n = int(word)\n",
        "        if 'in' in words[i:i+2] and i+2 < len(words):\n",
        "            region = words[i+2].capitalize()\n",
        "        for alias, col in COLUMN_ALIASES.items():\n",
        "            if alias in query and col in df.columns:\n",
        "                metric = col\n",
        "                break\n",
        "    metric = metric or \"Sales\"\n",
        "\n",
        "    # Filter data\n",
        "    filtered_df = df.copy()\n",
        "    if region and region in df['Region'].unique():\n",
        "        filtered_df = filtered_df[filtered_df['Region'] == region]\n",
        "    if filtered_df.empty:\n",
        "        return f\"No data found for {region}.\"\n",
        "\n",
        "    # Handle action\n",
        "    context = \"\"\n",
        "    if action == \"predict\" and inputs:\n",
        "        model_choice = inputs.get(\"model\")\n",
        "        if model_choice not in models:\n",
        "            return f\"Model {model_choice} not available.\"\n",
        "        try:\n",
        "            if model_choice in [\"Linear Regression\", \"XGBoost\", \"Random Forest\"]:\n",
        "                closest_row = find_closest_row(filtered_df, inputs[\"quantity\"], inputs[\"discount\"], inputs[\"fulfillment_time\"])\n",
        "                input_features = preprocess_input(closest_row, scalers[model_choice])\n",
        "                prediction = models[model_choice].predict(input_features)[0]\n",
        "            elif model_choice == \"ARIMA\":\n",
        "                prediction = models[\"ARIMA\"].forecast(steps=inputs.get(\"months\", 1))[-1]\n",
        "            elif model_choice == \"LSTM\":\n",
        "                input_lstm, scaler = preprocess_lstm_input(inputs[\"quantity\"], inputs[\"discount\"], inputs[\"fulfillment_time\"])\n",
        "                prediction_scaled = models[\"LSTM\"].predict(input_lstm, verbose=0)[0][0]\n",
        "                prediction = scaler.inverse_transform([[prediction_scaled, 0, 0]])[0][0]\n",
        "            context = f\"Predicted Sales: â‚¹{prediction:,.2f}\\nInputs: {inputs}\"\n",
        "        except Exception as e:\n",
        "            return f\"Prediction failed: {str(e)}\"\n",
        "    elif action == \"top\":\n",
        "        top_items = filtered_df.sort_values(metric, ascending=False).head(n)\n",
        "        if top_items[metric].isna().all():\n",
        "            return f\"No valid {metric} data found.\"\n",
        "        context = top_items.to_csv(index=False)\n",
        "    else:\n",
        "        context = filtered_df.describe().to_csv() + \"\\nSample:\\n\" + filtered_df.head(5).to_csv(index=False)\n",
        "\n",
        "    # LLM query\n",
        "    history_context = \"\\n\".join(\n",
        "        f\"Q: {chat['query']}\\nA: {chat['response']}\"\n",
        "        for chat in st.session_state.chat_history[-3:]\n",
        "    ) if st.session_state.chat_history else \"No prior context.\"\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"mistralai/mistral-7b-instruct:free\",\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You're a professional BI assistantâ€”deliver clear, concise insights based on data or predictions.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"History:\\n{history_context}\\nData/Prediction:\\n{context}\\nQuery: {query}\\nProvide actionable insights.\"}\n",
        "        ]\n",
        "    }\n",
        "    try:\n",
        "        with st.spinner(\"Analyzing...\"):\n",
        "            response = requests.post(API_URL, headers=HEADERS, json=payload)\n",
        "            response.raise_for_status()\n",
        "            answer = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
        "            st.session_state.chat_history.append({\"query\": query, \"response\": answer})\n",
        "            return answer\n",
        "    except Exception as e:\n",
        "        error_msg = f\"Failed to fetch insights: {str(e)}\"\n",
        "        st.session_state.chat_history.append({\"query\": query, \"response\": error_msg})\n",
        "        return error_msg\n",
        "\n",
        "# Load data and models\n",
        "df = load_data()\n",
        "models, scalers = load_models_and_scalers()\n",
        "\n",
        "# Initialize session state\n",
        "if 'chat_history' not in st.session_state:\n",
        "    st.session_state.chat_history = []\n",
        "\n",
        "# UI Layout\n",
        "st.title(\"Retail Analytics Dashboard\")\n",
        "st.markdown(\"Predict sales, uncover insights, and visualize trendsâ€”all in one place.\")\n",
        "\n",
        "# Three-column layout\n",
        "col1, col2, col3 = st.columns([2, 1, 1])\n",
        "\n",
        "# BI Query (Left)\n",
        "with col1:\n",
        "    st.subheader(\"Business Insights\")\n",
        "    with st.form(key='query_form', clear_on_submit=True):\n",
        "        query = st.text_input(\"Ask about your data\", placeholder=\"e.g., 'Top 5 sales in East', 'Summary of profit'\")\n",
        "        submit_query = st.form_submit_button(\"Get Insights\")\n",
        "\n",
        "    if submit_query and query:\n",
        "        response = process_query(query, df, models, scalers)\n",
        "        st.success(\"Insight Generated!\")\n",
        "        st.markdown(f\"**Answer:** {response}\")\n",
        "\n",
        "    st.subheader(\"Insight History\")\n",
        "    if st.session_state.chat_history:\n",
        "        for i, chat in enumerate(st.session_state.chat_history[::-1][:5]):\n",
        "            with st.expander(f\"Q: {chat['query']} (#{len(st.session_state.chat_history)-i})\"):\n",
        "                st.write(f\"A: {chat['response']}\")\n",
        "    else:\n",
        "        st.info(\"No insights yetâ€”ask away!\")\n",
        "\n",
        "# ML Prediction (Middle)\n",
        "with col2:\n",
        "    st.subheader(\"Sales Prediction\")\n",
        "    with st.form(key='predict_form'):\n",
        "        model_choice = st.selectbox(\"Model\", list(models.keys()))\n",
        "        quantity = st.slider(\"Quantity\", 1, 10, 1)\n",
        "        discount = st.slider(\"Discount\", 0.0, 1.0, 0.1, step=0.01)\n",
        "        fulfillment_time = st.slider(\"Fulfillment Time (days)\", 1, 10, 1)\n",
        "        region = st.selectbox(\"Region\", df['Region'].unique())\n",
        "        months = st.slider(\"Months to Forecast\", 1, 12, 1) if model_choice == \"ARIMA\" else 1\n",
        "        submit_predict = st.form_submit_button(\"Predict\")\n",
        "\n",
        "    if submit_predict:\n",
        "        inputs = {\n",
        "            \"model\": model_choice, \"quantity\": quantity, \"discount\": discount,\n",
        "            \"fulfillment_time\": fulfillment_time, \"region\": region, \"months\": months\n",
        "        }\n",
        "        response = process_query(f\"Predict sales in {region}\", df, models, scalers, inputs)\n",
        "        st.success(\"Prediction Generated!\")\n",
        "        st.markdown(f\"**Result:** {response}\")\n",
        "\n",
        "        # Quick viz for ML models\n",
        "        if model_choice in [\"Linear Regression\", \"XGBoost\", \"Random Forest\"]:\n",
        "            quantity_values = np.linspace(1, 10, 5)\n",
        "            predictions = []\n",
        "            for q in quantity_values:\n",
        "                closest_row = find_closest_row(df, q, discount, fulfillment_time)\n",
        "                input_features = preprocess_input(closest_row, scalers[model_choice])\n",
        "                predictions.append(models[model_choice].predict(input_features)[0])\n",
        "            fig, ax = plt.subplots(figsize=(4, 3))\n",
        "            ax.bar(quantity_values.astype(str), predictions, color=\"#1E90FF\")\n",
        "            ax.set_xlabel(\"Quantity\")\n",
        "            ax.set_ylabel(\"Sales (â‚¹)\")\n",
        "            ax.set_title(f\"{model_choice} Forecast\")\n",
        "            st.pyplot(fig)\n",
        "\n",
        "# Dashboard Stats (Right)\n",
        "with col3:\n",
        "    st.subheader(\"Data Snapshot\")\n",
        "    st.metric(\"Total Records\", f\"{len(df):,}\")\n",
        "    st.metric(\"Regions\", df['Region'].nunique())\n",
        "    st.metric(\"Total Sales\", f\"â‚¹{df['Sales'].sum():,.2f}\")\n",
        "    st.metric(\"Average Profit\", f\"â‚¹{df['Profit'].mean():,.2f}\")\n",
        "\n",
        "    # Dynamic chart\n",
        "    if st.session_state.chat_history:\n",
        "        last_query = st.session_state.chat_history[-1]['query']\n",
        "        metric = next((COLUMN_ALIASES[alias] for alias in COLUMN_ALIASES if alias in last_query.lower()), 'Sales')\n",
        "        top_5 = df.nlargest(5, metric)[['Product Name', metric]]\n",
        "        chart = alt.Chart(top_5).mark_bar().encode(\n",
        "            x=alt.X('Product Name', sort=None),\n",
        "            y=metric,\n",
        "            tooltip=['Product Name', metric]\n",
        "        ).properties(width=200, height=200, title=f\"Top 5 {metric}\")\n",
        "        st.altair_chart(chart)\n",
        "\n",
        "# Footer\n",
        "st.markdown(f\"Â© 2025 Retail Analytics Dashboard | Updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ],
      "metadata": {
        "id": "UkBkGVk6uASd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dashboard"
      ],
      "metadata": {
        "id": "GddnlL9Mt9mF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "from pyngrok import ngrok, conf\n",
        "import os\n",
        "\n",
        "# Set Ngrok authtoken\n",
        "NGROK_TOKEN = st.secrets.get(\"NGROK_TOKEN\", \"your-ngrok-token\")  # Set in Colab secrets\n",
        "try:\n",
        "    ngrok.set_auth_token(NGROK_TOKEN)\n",
        "    print(\"Ngrok authtoken configured.\")\n",
        "except Exception as e:\n",
        "    st.error(f\"Ngrok authtoken setup failed: {e}\")\n",
        "    st.stop()\n",
        "\n",
        "# Kill existing tunnels\n",
        "try:\n",
        "    for tunnel in ngrok.get_tunnels():\n",
        "        ngrok.disconnect(tunnel.public_url)\n",
        "    print(\"Cleared old tunnels.\")\n",
        "except Exception:\n",
        "    print(\"No tunnels to clear.\")\n",
        "\n",
        "# Run Streamlit\n",
        "def run_streamlit():\n",
        "    process = subprocess.Popen([\"streamlit\", \"run\", \"app.py\", \"--server.port\", \"8501\"])\n",
        "    process.wait()\n",
        "\n",
        "# Start Streamlit in background\n",
        "thread = threading.Thread(target=run_streamlit, daemon=True)\n",
        "thread.start()\n",
        "print(\"Starting Streamlit...\")\n",
        "\n",
        "# Wait for Streamlit to initialize\n",
        "time.sleep(5)\n",
        "\n",
        "# Create Ngrok tunnel\n",
        "try:\n",
        "    public_url = ngrok.connect(8501, proto=\"http\")\n",
        "    print(f\"Dashboard live at: {public_url}\")\n",
        "except Exception as e:\n",
        "    st.error(f\"Ngrok tunnel failed: {e}\")\n",
        "    st.stop()"
      ],
      "metadata": {
        "id": "BFFWLy_HuQt9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}